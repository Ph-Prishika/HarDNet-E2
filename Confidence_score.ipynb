{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your modified HarDNet repository path\n",
    "sys.path.append('./Pytorch-HarDNet_new_SE_resi')\n",
    "from hardnet import HarDNet\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "def get_model():\n",
    "    model = HarDNet(arch=85, pretrained=False)\n",
    "    model.base[-1][3] = nn.Linear(model.base[-1][3].in_features, NUM_CLASSES)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/app/.pth\"\n",
    "\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "print(\" Model weights loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19421957",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/app//\"\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(TEST_DIR, transform=test_transform)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "class_names = test_dataset.classes\n",
    "\n",
    "print(\"Test samples:\", len(test_dataset))\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89eafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_and_save(model, loader, class_names,\n",
    "                      save_path=\"/app/.csv\",\n",
    "                      overconf_threshold=0.9):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_paths = []\n",
    "\n",
    "    idx_counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            bsz = inputs.size(0)\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "\n",
    "            conf, preds = torch.max(probs, dim=1)\n",
    "\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "            batch_paths = [loader.dataset.samples[i][0]\n",
    "                           for i in range(idx_counter, idx_counter + bsz)]\n",
    "            all_paths.extend(batch_paths)\n",
    "            idx_counter += bsz\n",
    "\n",
    "    all_probs = np.concatenate(all_probs)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    confidences = all_probs.max(axis=1)\n",
    "    correctness = (all_preds == all_labels)\n",
    "\n",
    "    # Overconfidence condition\n",
    "    overconfident_errors = (~correctness) & (confidences >= overconf_threshold)\n",
    "\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame({\n",
    "        \"path\": all_paths,\n",
    "        \"y_true\": all_labels,\n",
    "        \"y_true_name\": [class_names[i] for i in all_labels],\n",
    "        \"y_pred\": all_preds,\n",
    "        \"y_pred_name\": [class_names[i] for i in all_preds],\n",
    "        \"confidence\": confidences,\n",
    "        \"correct\": correctness,\n",
    "        \"overconfident_error\": overconfident_errors\n",
    "    })\n",
    "\n",
    "    # Add class probabilities\n",
    "    for k, cname in enumerate(class_names):\n",
    "        df[f\"prob_{cname}\"] = all_probs[:, k]\n",
    "\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\" CSV saved to: {save_path}\")\n",
    "\n",
    "    # Print summary statistics\n",
    "    total_samples = len(df)\n",
    "    total_errors = (~correctness).sum()\n",
    "    total_overconf_errors = overconfident_errors.sum()\n",
    "\n",
    "    print(\"\\n Test Confidence Statistics\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(f\"Total test samples: {total_samples}\")\n",
    "    print(f\"Total misclassifications: {total_errors}\")\n",
    "    print(f\"Overconfident misclassifications (≥ {overconf_threshold}): {total_overconf_errors}\")\n",
    "\n",
    "    if total_errors > 0:\n",
    "        print(f\"Percentage of errors that are overconfident: \"\n",
    "              f\"{(total_overconf_errors / total_errors) * 100:.2f}%\")\n",
    "\n",
    "    print(f\"Percentage of total test samples that are overconfident errors: \"\n",
    "          f\"{(total_overconf_errors / total_samples) * 100:.2f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_test = evaluate_and_save(model, test_loader, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Overconfident Errors by True Class\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "for cname in class_names:\n",
    "    class_mask = (df_test[\"y_true_name\"] == cname)\n",
    "    overconf_class = df_test[class_mask & (df_test[\"overconfident_error\"])]\n",
    "    print(f\"{cname}: {len(overconf_class)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afc45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [0.8, 0.85, 0.9, 0.95]:\n",
    "    count = ((~df_test[\"correct\"]) & (df_test[\"confidence\"] >= t)).sum()\n",
    "    print(f\"Overconfident errors (≥ {t}): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718946c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a737a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
